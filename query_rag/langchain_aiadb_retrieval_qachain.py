# -*- coding: utf-8 -*-
"""Langchain_AIAdb_Retrieval_QAchain_github.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qDXmNksxIRcFJu-7OPXeubDCTk_4-SCJ
"""

# 此段程式碼，主要建立RAG流程使用的embedding model，並連線至AIA PrimeHub vectorDB
# 接著建立多個LLM Model服務與retrievers，建立PromptTemplate for RetrievalQA
# 最後建立RetrievalQA : qa_chain，其中qa_chain的{question}參數，為提供給前端進行query時的接口

# 1、建立Emmbedings model for query and retrieavl

from langchain_nomic.embeddings import NomicEmbeddings
os.environ['NOMIC_API_KEY'] = 'nk-BCjzlgQXfYdZNA6kUZ-6Jeq1NIG2JhlQJaTe9BedpDc'
# 確保跟PrimeHub vectorDB使用的模型一致：nomic-embed-text-v1.5
embedding = NomicEmbeddings(model="nomic-embed-text-v1.5")



# 2、建立連線至AIA PrimeHub vectorDB

import chromadb
from chromadb.config import Settings
from langchain.vectorstores import Chroma

# 建立PrimeHub vectorDB連線設定
httpClient = chromadb.HttpClient(
  host='64.176.47.89', port=8000,
  settings=Settings(chroma_client_auth_provider="chromadb.auth.basic_authn.BasicAuthClientProvider",chroma_client_auth_credentials="admin:admin")
  )

# 連線至各版本資料庫(collection name)，tell LangChain to use our client and collection name
db = Chroma(
    client=httpClient,
    collection_name="xt131028_v1",
    embedding_function=embedding, #使用步驟1 設定的embedding
)



# 3、建立多個LLM Model服務，以進行後續比較

from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from langchain.llms import OpenAI
from langchain_community.llms import Ollama

# 建立第一個模型服務：Call Groq_Llama3_8B，設定MODEL_NAME = "llama3-70b-8192" or "llama3-8b-8192"
llm_Groq_llama3 = ChatGroq(temperature=0, groq_api_key="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", model_name="llama3-8b-8192")

# 建立第二個模型服務：Call PrimeHub ngrok Llama3
llm_primehub_llama3 = Ollama(model="llama3", base_url="http://3ece-140-109-17-42.ngrok-free.app")



# 4、建立多種retriever，以下共有retriever、compression_retriever、MultiQuery_retriever

# 建立第一種retriever：使用最基本的Vector store-backed retriever ，使用向量存儲實現的搜索方法（如相似性搜索和MMR）來查詢向量存儲中的文字

retriever = db.as_retriever(search_kwargs={"fetch_k": 10, #使用MMR 設定最多检索10个相關文檔
                                           "k": 3, #並且返回最相關＆最多樣化的3個結果文檔
                                           "mmr_score_cache": True, #啟用MMR分數緩存
                                           "mmr_rerank_top_k": 30},#從相似度排序的前30個结果中選擇候選文檔進行MMR重新排序
                            retriever_mode="reduce_op_recursive",#啟用reduce_op_recursive檢索模式
                            # search_type 二擇一：search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.5} #以向量相似度（距離）去查詢vectordb，僅返回分數高於該閾值的文檔
                            search_type="mmr") #使用MMR搜索策略


# 建立第二種retriever：Contextual compression retriever 使用上、下文壓縮器

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers.document_compressors import LLMChainFilter

# 選擇要使用的LLM
# llm = llm_primehub_llama3
llm = llm_Groq_llama3
# 添加一個 LLMChainFilter，該壓縮器稍微簡單一些，但更強大，它使用LLM鏈來決定要過濾掉哪些最初檢索的文檔以及要返回哪些文檔
_filter = LLMChainFilter.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=_filter, base_retriever=retriever)


# 建立第三種retriever：MultiQueryRetriever

from langchain.retrievers.multi_query import MultiQueryRetriever
# 選擇要使用的LLM
# llm = llm_primehub_llama3
llm = llm_Groq_llama3
MultiQuery_retriever = MultiQueryRetriever.from_llm(
    retriever=db.as_retriever(), llm=llm
)



# 5、建立PromptTemplate for RetrieavlQA

from langchain.prompts import PromptTemplate
template = """Use the following pieces of context to answer the question at the end. Please answer in Chinese. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template)



# 6、建立RetrieavlQA : qa_chain

from langchain.chains import RetrievalQA
qa_chain = RetrievalQA.from_chain_type(llm=llm_Groq_llama3, #選擇要使用的 llm_Groq_llama3、llm_primehub_llama3
                                       retriever=retriever, # 擇一 retriever or compression_retriever or MultiQuery_retriever
                                       return_source_documents=True,
                                       chain_type="stuff",
                                       chain_type_kwargs={"prompt": QA_CHAIN_PROMPT})