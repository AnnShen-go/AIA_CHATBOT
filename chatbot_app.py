from langchain.chains import ConversationalRetrievalChain
from langchain.chains.llm import LLMChain
from langchain.schema import Document
from langchain_community.llms import Ollama

#import chromadb
#from chromadb.config import Settings
#from langchain.vectorstores import Chroma
import streamlit as st

__import__('pysqlite3')
import sys,os
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

#def get_chroma_collections(server_url):
    # Initialize the LangChain database client
    # é€£ç·šè¨­å®š
#    httpClient = chromadb.HttpClient(
#        host=server_url, port=8000,
#        settings=Settings(chroma_client_auth_provider="chromadb.auth.basic_authn.BasicAuthClientProvider",chroma_client_auth_credentials="admin:admin")
#   )
    # Get all collections from the Chroma Server
#    collections = httpClient.list_collections()

    #for c in collections:
    #    print(c)



#get_chroma_collections("64.176.47.89")
#ngrok http 11434 --host-header="localhost:11434" --scheme http

with st.sidebar:
    model_platform_type = st.radio(
        "è«‹é¸æ“‡æƒ³ç”¨çš„æ¨¡å‹å¹³å°",
        ["ollama", "open_ai"],
        captions = ["Ollama åœ°ç«¯å¹³å°", "Open AI é›²ç«¯å¹³å°"],
        label_visibility="hidden")

    st.divider()

    openai_api_key = ""
    llm_model = ""
    ollama_api_url = ""

    if model_platform_type == "ollama":
        llm_model = st.text_input("LLM æ¨¡å‹", key="llm_model", value="ycchen/breeze-7b-instruct-v1_0")
        ollama_api_url = st.text_input("Ollama API URL", key="ollama_api_url", value="http://8cf8-140-109-17-45.ngrok-free.app")
    else:   
        openai_api_key = st.text_input("OpenAI API Key", key="chatbot_api_key", type="password")
    #    "[Get an OpenAI API key](https://platform.openai.com/account/api-keys)"
    #    "[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)"
    #    "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"
    #collection_version = st.text_input("Choose collection version")
st.title("ğŸ’¬ AIA èª²ç¨‹å°å¹«æ‰‹")
st.caption("ğŸš€ AIA Course Assistant")
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "å—¨! æˆ‘æ˜¯ AIA å°ç£äººå·¥æ™ºæ…§å­¸æ ¡çš„è™›æ“¬åŠ©ç†ï¼Œéš¨æ™‚æº–å‚™å›ç­”æ‚¨çš„èª²ç¨‹å•é¡Œ"}]

if 'button_disabled' not in st.session_state:
    st.session_state.button_disabled = False

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# Example prompts
example_prompts = [
    "AIA æœ‰ä»€éº¼èª²ç¨‹?",
    "è«‹ä»‹ç´¹ç¶“ç†äººå°ˆç­",
    "æœ‰ LLM ç›¸é—œçš„èª²ç¨‹å—?",
    "è«‹çµ¦æˆ‘æŠ€è¡“é ˜è¢–ç­çš„èª²ç¨‹ç°¡ç« "
]

def run_rag_process(prompt):

    if model_platform_type == 'ollama':
        if llm_model == "" or ollama_api_url == "":
            st.warning('è«‹è¼¸å…¥ LLM æ¨¡å‹åç¨±åŠ Ollama API URL', icon="âš ï¸")
            return
    elif openai_api_key == "":
        st.warning('è«‹è¼¸å…¥ OpenAI API Key', icon="âš ï¸")
        return

    from query_rag.qa_chain_module import QARetrievalPipeline

    # åˆå§‹åŒ–æµæ°´ç·šï¼Œå‚³å…¥å¿…è¦çš„å‚æ•°
    pipeline = QARetrievalPipeline(
        collection_name="stella_400_18admission_and_qa",
        nomic_api_key="nk-BCjzlgQXfYdZNA6kUZ-6Jeq1NIG2JhlQJaTe9BedpDc", #æ›æˆè‡ªå·±çš„nomic_api_key
        chroma_host="64.176.47.89",
        chroma_port=8000,
        groq_api_key="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",#æ›æˆè‡ªå·±çš„groq_api_key
        groq_model_name="llama3-8b-8192",
        ollama_model=llm_model,
        ollama_base_url=ollama_api_url, # è¨­å®šè‡ªå»ºçš„LLMæœå‹™ä½ç½®
        openai_api_key=openai_api_key,
        openai_model_name="gpt-4o"
    )

    # é¸æ“‡è¦ä½¿ç”¨çš„ LLM æ¨¡å‹ï¼ˆTrue ä½¿ç”¨ PrimeHub Ollama æ¨¡å‹ï¼ŒFalse ä½¿ç”¨ Groq æ¨¡å‹ï¼‰
    if model_platform_type == 'ollama':
        pipeline.set_llm(use_primehub=True)
    else:
        pipeline.set_llm(use_openai=True)

    pipeline.set_retriever("default")

    response = pipeline.query(prompt)
    return response

clicked_button_text = None

cols = st.columns(4)

for i, button_text in enumerate(example_prompts):
    with cols[i]:
        if st.button(button_text, disabled=st.session_state.button_disabled):
            clicked_button_text = button_text
            st.session_state.button_disabled = True

# è™•ç†æŒ‰éˆ•é»æ“Šäº‹ä»¶
if clicked_button_text:
    st.session_state.messages.append({"role": "user", "content": clicked_button_text})
    with st.chat_message("user"):
        st.write(clicked_button_text)
    response = run_rag_process(clicked_button_text)  # èª¿ç”¨å›æ‡‰å‡½æ•¸
    if response:
        print(response)
        msg = response["result"]
        st.session_state.messages.append({"role": "assistant", "content": msg})
        with st.chat_message("assistant"):
            st.write(msg)
    # é‡æ–°å•Ÿç”¨æŒ‰éˆ•
    st.session_state.button_disabled = False        
    st.experimental_rerun()


if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    print(st.session_state.messages)
    response = run_rag_process(prompt)
    #response = llm.invoke(st.session_state.messages)
    #msg = response.choices[0].message.content
    if response != None:
        msg = response["result"]
        st.session_state.messages.append({"role": "assistant", "content": msg})
        st.chat_message("assistant").write(msg)

