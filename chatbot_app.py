from langchain.chains import ConversationalRetrievalChain
from langchain.chains.llm import LLMChain
from langchain.schema import Document
from langchain_community.llms import Ollama

#import chromadb
#from chromadb.config import Settings
#from langchain.vectorstores import Chroma
import streamlit as st

__import__('pysqlite3')
import sys,os
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

#def get_chroma_collections(server_url):
    # Initialize the LangChain database client
    # é€£ç·šè¨­å®š
#    httpClient = chromadb.HttpClient(
#        host=server_url, port=8000,
#        settings=Settings(chroma_client_auth_provider="chromadb.auth.basic_authn.BasicAuthClientProvider",chroma_client_auth_credentials="admin:admin")
#   )
    # Get all collections from the Chroma Server
#    collections = httpClient.list_collections()

    #for c in collections:
    #    print(c)



#get_chroma_collections("64.176.47.89")
#ngrok http 11434 --host-header="localhost:11434" --scheme http

with st.sidebar:
    model_platform_type = st.radio(
        "è«‹é¸æ“‡æƒ³ç”¨çš„æ¨¡å‹å¹³å°",
        ["ollama", "open_ai"],
        captions = ["Ollama åœ°ç«¯å¹³å°", "Open AI é›²ç«¯å¹³å°"],
        label_visibility="hidden")

    st.divider()

    if model_platform_type == "ollama":
        llm_model = st.text_input("LLM æ¨¡å‹", key="llm_model")
        ollama_api_url = st.text_input("Ollama API URL", key="ollama_api_url")
    else:   
        openai_api_key = st.text_input("OpenAI API Key", key="chatbot_api_key", type="password")
    #    "[Get an OpenAI API key](https://platform.openai.com/account/api-keys)"
    #    "[View the source code](https://github.com/streamlit/llm-examples/blob/main/Chatbot.py)"
    #    "[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/streamlit/llm-examples?quickstart=1)"
    #collection_version = st.text_input("Choose collection version")
st.title("ğŸ’¬ AIA Chatbot")
st.caption("ğŸš€ AIA èª²ç¨‹æŸ¥è©¢æ©Ÿå™¨äºº")
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

def run_rag_process(prompt):

    if model_platform_type == 'ollama':
        if llm_model == "" or ollama_api_url == "":
            st.warning('è«‹è¼¸å…¥ LLM æ¨¡å‹åç¨±åŠ Ollama API URL', icon="âš ï¸")
            return
    elif openai_api_key == "":
        st.warning('è«‹è¼¸å…¥ OpenAI API Key', icon="âš ï¸")
        return

    from query_rag.qa_chain_module import QARetrievalPipeline

    # åˆå§‹åŒ–æµæ°´ç·šï¼Œå‚³å…¥å¿…è¦çš„å‚æ•°
    pipeline = QARetrievalPipeline(
        collection_name="stella_400_18admission_and_qa",
        nomic_api_key="nk-BCjzlgQXfYdZNA6kUZ-6Jeq1NIG2JhlQJaTe9BedpDc", #æ›æˆè‡ªå·±çš„nomic_api_key
        chroma_host="64.176.47.89",
        chroma_port=8000,
        groq_api_key="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",#æ›æˆè‡ªå·±çš„groq_api_key
        groq_model_name="llama3-8b-8192",
        ollama_model=llm_model,
        ollama_base_url=ollama_api_url, # è¨­å®šè‡ªå»ºçš„LLMæœå‹™ä½ç½®
        openai_api_key=openai_api_key,
        openai_model_name="gpt-4o"
    )

    # é¸æ“‡è¦ä½¿ç”¨çš„ LLM æ¨¡å‹ï¼ˆTrue ä½¿ç”¨ PrimeHub Ollama æ¨¡å‹ï¼ŒFalse ä½¿ç”¨ Groq æ¨¡å‹ï¼‰
    if model_platform_type == 'ollama':
        pipeline.set_llm(use_primehub=True)
    else:
        pipeline.set_llm(use_openai=True)

    response = pipeline.query(prompt)
    return response


if prompt := st.chat_input():
    print(f"model = ${llm_model}")
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    print(st.session_state.messages)
    response = run_rag_process(prompt)
    #response = llm.invoke(st.session_state.messages)
    #msg = response.choices[0].message.content
    if response != None:
        msg = response["result"]
        st.session_state.messages.append({"role": "assistant", "content": msg})
        st.chat_message("assistant").write(msg)

